{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c99b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57605387-d20f-4b83-a331-67ec6c0f28a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import string\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from constants import inferences_folder, results_folder\n",
    "\n",
    "# Load the pre-trained sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "def parse_plausible_answers(text):\n",
    "    \"\"\"\n",
    "    Parses a string that represents a set of plausible answers into a Python set.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): A string that looks like a Python set of strings (e.g., \"{'no', 'yes'}\")\n",
    "\n",
    "    Returns:\n",
    "    - set: A set containing the plausible answers.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return ast.literal_eval(text)\n",
    "    except ValueError:\n",
    "        return set()\n",
    "\n",
    "\n",
    "def extract_answer(text):\n",
    "    \"\"\"\n",
    "    Extracts the answer from a generated answer text string that follows the format:\n",
    "    '...Answer:  motorcycle'\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The complete text from which to extract the answer.\n",
    "\n",
    "    Returns:\n",
    "    - str: The extracted answer, trimmed and converted to lowercase.\n",
    "    \"\"\"\n",
    "    # Find the index of the \"Answer:\" substring and extract everything after it\n",
    "    answer_start = text.rfind(':') + 1\n",
    "\n",
    "    answer = text[answer_start:].strip().lower()\n",
    "    answer = answer.split(\" \")[0].strip(string.punctuation)\n",
    "    return answer\n",
    "\n",
    "    # print(f\"Text answer: '{text[answer_start:]}'\")\n",
    "    # if answer_start >= 0:\n",
    "    #     # Extract the answer and split by any new lines or extraneous text\n",
    "        \n",
    "    #     print(f\"gen answer:\\n'{answer}'\",)\n",
    "    #     return answer\n",
    "    # return 'NA'\n",
    "\n",
    "def evaluate_exact_match(data):\n",
    "    def is_correct(row):\n",
    "        # print(row['cleaned_generated_answer'])\n",
    "        # print(\"original answer : \", row['Answer'])\n",
    "        # print(row['plausible_answers_set'])\n",
    "        return row['cleaned_generated_answer'] in row['plausible_answers_set']\n",
    "    correct_count = data.apply(is_correct, axis=1).sum()\n",
    "    total = len(data)\n",
    "    return correct_count / total\n",
    "\n",
    "def evaluate_semantic_similarity(data, threshold=0.7):\n",
    "\n",
    "    def is_semantically_correct(row):\n",
    "        generated_answer_embedding = model.encode(row['cleaned_generated_answer'], convert_to_tensor=True)\n",
    "        plausible_answers_embeddings = model.encode(list(row['plausible_answers_set']), convert_to_tensor=True)\n",
    "\n",
    "        # Compute cosine similarities and check if any are above the threshold\n",
    "        similarities = util.pytorch_cos_sim(generated_answer_embedding, plausible_answers_embeddings)\n",
    "        max_similarity = np.max(similarities.cpu().numpy())\n",
    "        return max_similarity >= threshold\n",
    "\n",
    "    correct_counts = data.apply(is_semantically_correct, axis=1).sum()\n",
    "    total = len(data)\n",
    "    return correct_counts / total\n",
    "\n",
    "\n",
    "def compute_accuracies(data):\n",
    "    # Exact Match Accuracy\n",
    "    exact_match_accuracy = evaluate_exact_match(data)\n",
    "\n",
    "    # Semantic Similarity Accuracy\n",
    "    semantic_similarity_accuracy = evaluate_semantic_similarity(data)\n",
    "\n",
    "    return {\n",
    "        'exact_match_accuracy': exact_match_accuracy,\n",
    "        'semantic_similarity_accuracy': semantic_similarity_accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "def split_and_evaluate(data):\n",
    "    # Split the data based on answer type\n",
    "    types = ['other', 'yes/no', 'number']\n",
    "    results = {}\n",
    "    total_count = 0\n",
    "\n",
    "    for answer_type in types:\n",
    "        subset = data[data['Answer Type'] == answer_type]\n",
    "        if len(subset) > 0:\n",
    "            accuracies = compute_accuracies(subset)\n",
    "            results[answer_type] = {\n",
    "                'accuracies': accuracies,\n",
    "                'count': len(subset)\n",
    "            }\n",
    "            total_count += len(subset)\n",
    "        else:\n",
    "            results[answer_type] = {\n",
    "                'accuracies': {\n",
    "                    'exact_match_accuracy': None,\n",
    "                    'semantic_similarity_accuracy': None\n",
    "                },\n",
    "                'count': 0\n",
    "            }\n",
    "\n",
    "    # Compute weighted accuracies\n",
    "    weighted_exact_match = sum(\n",
    "        info['accuracies']['exact_match_accuracy'] * info['count'] for info in results.values() if\n",
    "        info['accuracies']['exact_match_accuracy'] is not None) / total_count\n",
    "    weighted_semantic = sum(\n",
    "        info['accuracies']['semantic_similarity_accuracy'] * info['count'] for info in results.values() if\n",
    "        info['accuracies']['semantic_similarity_accuracy'] is not None) / total_count\n",
    "\n",
    "    overall_accuracies = {\n",
    "        'weighted_exact_match_accuracy': weighted_exact_match,\n",
    "        'weighted_semantic_accuracy': weighted_semantic\n",
    "    }\n",
    "\n",
    "    # Combine results and overall accuracies\n",
    "    results['overall'] = overall_accuracies\n",
    "\n",
    "    return results\n",
    "\n",
    "def load_and_merge_data(ground_truth_csv, generated_csv):\n",
    "    # Load the data from CSV files\n",
    "    ground_truth_data = pd.read_csv(ground_truth_csv)\n",
    "    generated_data = pd.read_csv(generated_csv)\n",
    "\n",
    "    # Clean the 'Generated Answer' in the generated data\n",
    "    generated_data['cleaned_generated_answer'] = generated_data['Generated Answer'].apply(extract_answer)\n",
    "\n",
    "    # Parse the plausible answers in the ground truth data\n",
    "    ground_truth_data['plausible_answers_set'] = ground_truth_data['Plausible answers'].apply(parse_plausible_answers)\n",
    "\n",
    "    # Merge the datasets on 'Image ID' and 'Question' using an inner join\n",
    "    merged_data = pd.merge(ground_truth_data, generated_data, how='inner', on=['Image ID', 'Question'], suffixes=('_truth', '_generated'))\n",
    "\n",
    "    # Optionally, print columns and some data for verification\n",
    "    # print(\"Columns in merged dataset:\", merged_data.columns.values.tolist())\n",
    "    # print(\"Sample 'Image ID' data from merged dataset:\", merged_data['Image ID'].head())\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "def save_results_to_json(filename, results):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "def find_csv_files(directory, suffix='.csv'):\n",
    "    \"\"\"\n",
    "    Finds all files in the specified directory with the given suffix.\n",
    "\n",
    "    Parameters:\n",
    "    - directory (str): The directory to search for files.\n",
    "    - suffix (str): The file suffix to search for.\n",
    "\n",
    "    Returns:\n",
    "    - list of str: A list of full file paths that match the suffix.\n",
    "    \"\"\"\n",
    "    return [os.path.join(directory, file) for file in os.listdir(directory) if file.endswith(suffix)]\n",
    "\n",
    "\n",
    "def process_files(ground_truth_csv, generated_files, destination_folder):\n",
    "    for generated_csv in tqdm(generated_files, desc=\"Processing files\"):\n",
    "        print(f\"Processing File : {generated_csv}\\n\")\n",
    "        # Extract the base filename without '10k' and with modified suffix\n",
    "        # base_filename = os.path.basename(generated_csv).replace('10k_', '').replace('.csv', '_results.json')\n",
    "\n",
    "        base_filename = os.path.basename(generated_csv).replace('.csv', '_results.json')\n",
    "\n",
    "        # Load and merge data\n",
    "        merged_data = load_and_merge_data(ground_truth_csv, generated_csv)\n",
    "\n",
    "        merged_data.to_csv(\"dummy.csv\", index = True)\n",
    "\n",
    "        # Evaluate and compute results\n",
    "        evaluation_results = split_and_evaluate(merged_data)\n",
    "\n",
    "        # Save to JSON file with the new filename\n",
    "        save_results_to_json(f\"{destination_folder}/{base_filename}\", evaluation_results)\n",
    "        print(f\"Results saved to {destination_folder}/{base_filename}\")\n",
    "\n",
    "# Constant ground truth file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9130ff2b-e6b6-44fb-ab60-7c665aff207e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_csv = '../data/10k_mapped_question_answers.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ca8ab1a-0757-4b19-8fab-d555cf96d8bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('blip_llama', '../inferences\\\\blip_llama'),\n",
       " ('blip_mistral', '../inferences\\\\blip_mistral'),\n",
       " ('blip_yolo_llama', '../inferences\\\\blip_yolo_llama'),\n",
       " ('blip_yolo_llama_quantized_templates',\n",
       "  '../inferences\\\\blip_yolo_llama_quantized_templates'),\n",
       " ('blip_yolo_llama_unquantized_templates',\n",
       "  '../inferences\\\\blip_yolo_llama_unquantized_templates'),\n",
       " ('blip_yolo_mistral', '../inferences\\\\blip_yolo_mistral'),\n",
       " ('blip_yolo_mistral_quantized_templates',\n",
       "  '../inferences\\\\blip_yolo_mistral_quantized_templates'),\n",
       " ('blip_yolo_mistral_unquantized_templates',\n",
       "  '../inferences\\\\blip_yolo_mistral_unquantized_templates'),\n",
       " ('yolo_llama', '../inferences\\\\yolo_llama'),\n",
       " ('yolo_mistral', '../inferences\\\\yolo_mistral')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folders = [(folder, os.path.join(inferences_folder, folder)) for folder in os.listdir(inferences_folder) if not os.path.isfile(os.path.join(inferences_folder, folder)) and not folder.startswith(\".\") ]\n",
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "079dcb29-be48-4483-b16e-b33cd477a3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(filter(lambda x: x.contains(\"quantized\"), folders))\n",
    "# subset = list(filter(lambda x: \"quant\" in x[0] in x[0], folders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9957ea21-3832-4b62-b85e-065393cb30b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for folder, folder_path in subset:\n",
    "#     generated_files = find_csv_files(folder_path)\n",
    "#     for file in generated_files:\n",
    "#         df= pd.read_csv(file)\n",
    "#         df.rename({\"Model Output\": \"Generated Answer\", \n",
    "#                   \"Generated Caption\": \"Caption\", \n",
    "#                   \"Generated Detections\": \"Detections\"}, axis=1, inplace=True)\n",
    "#         df.drop([\"Answer Type\", \"Question Type\", \"Answer\", \"Plausible answers\", \"Image file\"], axis=1, inplace=True)\n",
    "#         df.to_csv(file, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33beeb1b-54a3-4b69-9e2c-4f4fd66ae50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for folder, folder_path in subset:\n",
    "#     # folder= f\"{inferences_folder}/{folder}\"\n",
    "#     # List of generated files\n",
    "#     generated_files = find_csv_files(folder_path)\n",
    "#     process_files(ground_truth_csv, generated_files, f\"{results_folder}/{folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6e4c489-936d-466f-86b4-0abd053e4ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing File : ../inferences/yolo_llama\\10k_enhanced_prompt_with_generation_cfg_answers.csv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  33%|███▎      | 1/3 [00:26<00:52, 26.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ../results/yolo_llama/10k_enhanced_prompt_with_generation_cfg_answers_results.json\n",
      "Processing File : ../inferences/yolo_llama\\10k_generation_cfg_answers.csv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  67%|██████▋   | 2/3 [00:54<00:27, 27.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ../results/yolo_llama/10k_generation_cfg_answers_results.json\n",
      "Processing File : ../inferences/yolo_llama\\10k_generation_cfg_prompt_restriction_answers.csv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 3/3 [01:26<00:00, 28.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ../results/yolo_llama/10k_generation_cfg_prompt_restriction_answers_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing File : ../inferences/yolo_mistral\\10k_default_answers.csv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  25%|██▌       | 1/4 [05:47<17:21, 347.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ../results/yolo_mistral/10k_default_answers_results.json\n",
      "Processing File : ../inferences/yolo_mistral\\10k_enhanced_prompt_with_generation_cfg_answers.csv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  50%|█████     | 2/4 [11:40<11:41, 350.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ../results/yolo_mistral/10k_enhanced_prompt_with_generation_cfg_answers_results.json\n",
      "Processing File : ../inferences/yolo_mistral\\10k_generation_cfg_answers.csv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  75%|███████▌  | 3/4 [17:19<05:45, 345.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ../results/yolo_mistral/10k_generation_cfg_answers_results.json\n",
      "Processing File : ../inferences/yolo_mistral\\10k_generation_cfg_prompt_restriction_answers.csv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 4/4 [22:12<00:00, 333.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ../results/yolo_mistral/10k_generation_cfg_prompt_restriction_answers_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "folders = [('yolo_llama', '../inferences/yolo_llama'), ('yolo_mistral', '../inferences/yolo_mistral')]\n",
    "for folder, folder_path in folders:\n",
    "    # folder= f\"{inferences_folder}/{folder}\"\n",
    "    # List of generated files\n",
    "    generated_files = find_csv_files(folder_path)\n",
    "    process_files(ground_truth_csv, generated_files, f\"{results_folder}/{folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd5a5a0-c9d8-453b-848b-3fff7f33096b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99526fe6-c348-4eb5-b642-e55d2b53f8e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
