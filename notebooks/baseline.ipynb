{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base line model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 1.13.1\n",
      "Uninstalling torch-1.13.1:\n",
      "  Successfully uninstalled torch-1.13.1\n",
      "Found existing installation: torchvision 0.14.1\n",
      "Uninstalling torchvision-0.14.1:\n",
      "  Successfully uninstalled torchvision-0.14.1\n",
      "Found existing installation: torchaudio 0.8.1\n",
      "Uninstalling torchaudio-0.8.1:\n",
      "  Successfully uninstalled torchaudio-0.8.1\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall -y torch torchvision torchaudio \n",
    "# huggingface transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting huggingface\n",
      "  Using cached huggingface-0.0.1-py3-none-any.whl (2.5 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.8 MB 2.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch\n",
      "  Downloading torch-2.2.2-cp39-cp39-manylinux1_x86_64.whl (755.5 MB)\n",
      "\u001b[K     |███████████████                 | 351.9 MB 104.1 MB/s eta 0:00:04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████    | 663.0 MB 102.7 MB/s eta 0:00:01     |███████████████████████████▉    | 657.6 MB 102.7 MB/s eta 0:00:01"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 755.5 MB 32 kB/s \n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading torchvision-0.17.2-cp39-cp39-manylinux1_x86_64.whl (6.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.9 MB 86.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /shared/centos7/anaconda3/2022.05/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /shared/centos7/anaconda3/2022.05/lib/python3.9/site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: filelock in /shared/centos7/anaconda3/2022.05/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Collecting tokenizers<0.19,>=0.14\n",
      "  Downloading tokenizers-0.15.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.6 MB 86.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /shared/centos7/anaconda3/2022.05/lib/python3.9/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /shared/centos7/anaconda3/2022.05/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /shared/centos7/anaconda3/2022.05/lib/python3.9/site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: requests in /shared/centos7/anaconda3/2022.05/lib/python3.9/site-packages (from transformers) (2.27.1)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 58.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.19.3\n",
      "  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "\u001b[K     |████████████████████████████████| 388 kB 52.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[K     |██████████████████████▊         | 291.0 MB 104.4 MB/s eta 0:00:02"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 410.6 MB 10 kB/s /s eta 0:00:01     |██████████████████████████████▍ | 390.0 MB 102.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 124.2 MB 107.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 196.0 MB 106.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting triton==2.2.0\n",
      "  Downloading triton-2.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 167.9 MB 101 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: networkx in /shared/centos7/anaconda3/2022.05/lib/python3.9/site-packages (from torch) (2.7.1)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[K     |████████████████████████████████| 823 kB 40.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 731.7 MB 22 kB/s  eta 0:00:0101\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 23.7 MB 36.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[K     |████████████████████████████████| 99 kB 1.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting typing-extensions>=4.8.0\n",
      "  Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 121.6 MB 72 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 166.0 MB 93 kB/s  eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: sympy in /shared/centos7/anaconda3/2022.05/lib/python3.9/site-packages (from torch) (1.10.1)\n",
      "Requirement already satisfied: fsspec in /shared/centos7/anaconda3/2022.05/lib/python3.9/site-packages (from torch) (2022.2.0)\n",
      "Requirement already satisfied: jinja2 in /shared/centos7/anaconda3/2022.05/lib/python3.9/site-packages (from torch) (2.11.3)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 56.5 MB 68.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.1 MB 41.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 21.1 MB 48.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /shared/centos7/anaconda3/2022.05/lib/python3.9/site-packages (from torchvision) (9.0.1)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "\u001b[K     |████████████████████████████████| 171 kB 39.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /shared/centos7/anaconda3/2022.05/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /shared/centos7/anaconda3/2022.05/lib/python3.9/site-packages (from jinja2->torch) (2.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /shared/centos7/anaconda3/2022.05/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /shared/centos7/anaconda3/2022.05/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /shared/centos7/anaconda3/2022.05/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /shared/centos7/anaconda3/2022.05/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: mpmath>=0.19 in /shared/centos7/anaconda3/2022.05/lib/python3.9/site-packages (from sympy->torch) (1.2.1)\n",
      "Installing collected packages: nvidia-nvjitlink-cu12, typing-extensions, nvidia-cusparse-cu12, nvidia-cublas-cu12, fsspec, triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusolver-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, huggingface-hub, torch, tokenizers, safetensors, transformers, torchvision, huggingface\n",
      "Successfully installed fsspec-2024.3.1 huggingface-0.0.1 huggingface-hub-0.22.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 safetensors-0.4.2 tokenizers-0.15.2 torch-2.2.2 torchvision-0.17.2 transformers-4.39.3 triton-2.2.0 typing-extensions-4.11.0\n",
      "/bin/bash: nvcc: command not found\n"
     ]
    }
   ],
   "source": [
    "#### CPU setup\n",
    "# !pip install huggingface transformers torch torchvision\n",
    "\n",
    "#### GPU setup\n",
    "!nvcc --version\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu112/torch_stable.html\n",
    "# !pip install torch==1.13.1 torchvision==0.9.1 torchaudio===0.8.1 -f https://download.pytorch.org/whl/cu121/torch_stable.html\n",
    "    \n",
    "# !pip install torch==2.2.1+cu121 torchaudio==2.2.1+cu121 torchdata==0.7.1 torchsummary==1.5.1 torchtext==0.17.1 torchvision==0.17.1+cu121 tornado==6.3.3 tqdm==4.66.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import pipeline  # Use a pipeline as a high-level helper\n",
    "import os # For all os level functions\n",
    "import json # For parsing the json files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Util functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup the questions and answers for the image files\n",
    "def obtain_questions_for_image_ids(image_ids: list, question_file_path: str= \"questions.json\"):\n",
    "    \"\"\"This function obtains the questions for the list of the image ids passed from the file\"\"\"\n",
    "    with open(question_file_path) as file:\n",
    "        questions_data = json.load(file)\n",
    "    result = []\n",
    "    for question_details in questions_data['questions']:\n",
    "        if question_details['image_id'] in image_ids:\n",
    "            result.append(question_details)\n",
    "    return result\n",
    "\n",
    "\n",
    "def obtain_annotations_for_image_ids(image_ids: list, annotations_file_path: str = \"annotations.json\"):\n",
    "    \"\"\"This function obtians the annotations for the list of the image ids passed from the file\"\"\"\n",
    "    with open(annotations_file_path, 'r') as file:\n",
    "        annotations_data = json.load(file)\n",
    "    result = []\n",
    "    for annotation_details in annotations_data['annotations']:\n",
    "        if annotation_details['image_id'] in image_ids:\n",
    "            result.append(annotation_details)\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_refined_results_from_lists(relevant_questions:list, relevant_annotations:list):\n",
    "    \"\"\"This provides refined format of the question, image, plausible answers and other meta info\"\"\"\n",
    "    result = []\n",
    "    for question_info in relevant_questions:\n",
    "        q_id = question_info['question_id']\n",
    "        question = question_info['question']\n",
    "        # print(f\"Question ID: {q_id}, Question: {question}\")\n",
    "        for annotation_info in relevant_annotations:\n",
    "            if annotation_info['question_id'] == q_id:\n",
    "                answer = annotation_info[\"multiple_choice_answer\"]\n",
    "                plausile_answers = set(x['answer'] for x in annotation_info['answers'])\n",
    "                result.append({\"question_id\": q_id, \"question\": question, \n",
    "                               \"answer\": answer, \"plausible_answers\": plausile_answers,\n",
    "                               \"image_id\": annotation_info['image_id'], \"question_type\" :annotation_info['question_type'],\n",
    "                               \"answer_type\": annotation_info['answer_type']})\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_results(image_ids: list, folder: str):\n",
    "    relevant_annotations = obtain_annotations_for_image_ids(image_ids, annotations_file_path=f\"{folder}/annotations.json\")\n",
    "    relevant_questions = obtain_questions_for_image_ids(image_ids, question_file_path=f\"{folder}/questions.json\")\n",
    "    return get_refined_results_from_lists(relevant_questions=relevant_questions, relevant_annotations=relevant_annotations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_folder = \"../data/validation\"\n",
    "validation_images_folder = f\"{validation_folder}/images\"\n",
    "\n",
    "validation_image_files = [os.path.join(validation_images_folder, x) for x in os.listdir(validation_images_folder)]\n",
    "\n",
    "sample_validation_image_files = validation_image_files[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/validation/images/COCO_val2014_000000000042.jpg',\n",
       " '../data/validation/images/COCO_val2014_000000000073.jpg',\n",
       " '../data/validation/images/COCO_val2014_000000000074.jpg']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_validation_image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42, 73, 74]\n"
     ]
    }
   ],
   "source": [
    "image_files = sample_validation_image_files\n",
    "image_ids = [int(image_file.split(os.sep)[-1].split(\".\")[0].split(\"_\")[-1]) for image_file in image_files]\n",
    "image_ids\n",
    "\n",
    "relevant_annotations = obtain_annotations_for_image_ids(image_ids, annotations_file_path=f\"{validation_folder}/annotations.json\")\n",
    "relevant_questions = obtain_questions_for_image_ids(image_ids, question_file_path=f\"{validation_folder}/questions.json\")\n",
    "\n",
    "results = get_results(image_ids, validation_folder)\n",
    "print(image_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the baseline performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samavedam.m/.local/lib/python3.9/site-packages/transformers/generation/utils.py:1132: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Obtain the captions for these image files\n",
    "from transformers import pipeline\n",
    "blipPipe = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-large\")\n",
    "generated_captions = blipPipe(image_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the generated text along with image_id, image_file\n",
    "captions = [x[0]['generated_text'] for x in generated_captions]\n",
    "\n",
    "lookup = dict()\n",
    "for caption, image_id, image_file in zip(captions, image_ids, image_files):\n",
    "    lookup[image_id]= (image_file, caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/samavedam.m/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "### Hugging Face token for running the application\n",
    "\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "token = \"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\"\n",
    "\n",
    "login(token=token) ## This is bound to fail, add your token from chat and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a132c96e32134cbbbfbab4b950a5795a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "llamaPipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-2-7b-chat-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Zero Shot Performance\n",
    "from tqdm import tqdm\n",
    "\n",
    "number_of_samples = 10\n",
    "sampled_results = results[number_of_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:33<04:58, 33.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there is a dog that is laying on a rack with shoes, Answer the question 'What color are the gym shoes?' \\n Answer:  Black.\\n\\nExplanation:  Based on the image, the gym shoes are black.\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [01:33<06:34, 49.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there is a dog that is laying on a rack with shoes, Answer the question 'Is there a red sandal here?' \\n Answer:  Yes.\\n\\nExplanation:  In the image, there is a red sandal on the rack next to the dog.\\n\\nPlease provide the actual image and I will be happy to help you further.\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [04:22<12:06, 103.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': 'Based on the image caption (generated by BLIP model): there is a dog that is laying on a rack with shoes, Answer the question \\'What color is the flip flop?\\' \\n Answer:  Black.\\n\\nExplanation: \\nBased on the image caption (generated by BLIP model): there is a dog that is laying on a rack with shoes, Answer the question \\'What color is the flip flop?\\'  Answer:  Black.\\n\\nReasoning: \\nThe caption of the image generated by BLIP model mentions \"shoes\", which implies that there are multiple shoes present in the image. One of the shoes is described as \"black\", which indicates that it is black in color. Therefore, the answer to the question \"What color is the flip flop?\" is black.'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [08:14<15:27, 154.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': 'Based on the image caption (generated by BLIP model): araffe dog laying on the ground next to a bike on a city street, Answer the question \\'Does this dog have a collar?\\' \\n Answer:  Yes, the dog has a collar. \\n\\nIn this example, the BLIP model generates a textual description of the image, which is then used to answer a question about the image. The answer is based on the presence of a collar on the dog in the image.\\n\\nSimilarly, in natural language processing, a model like BLIP can be trained to generate textual descriptions of images, and these descriptions can be used to answer questions about the images. For example, a model like BLIP could be trained to generate descriptions of images of animals, and then used to answer questions like \"What type of animal is in this image?\" or \"Is this animal a mammal?\"\\n\\nIn summary, BLIP is a text-to-image model that can generate images based on textual descriptions, and it can also be used to answer questions about the generated images.'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [08:45<09:08, 109.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): araffe dog laying on the ground next to a bike on a city street, Answer the question 'Where is the dog laying?' \\n Answer:  'Next to a bike on a city street' \\n\\n\\n\\n\\n\\n\\n\\n\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [09:00<05:10, 77.69s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): araffe dog laying on the ground next to a bike on a city street, Answer the question 'What is the dog doing?' \\n Answer:  The dog is laying down.\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [17:40<11:07, 222.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there are two motorcycles parked next to each other on the street, Answer the question 'What is the license number?' \\n Answer: 4321234567.\\n\\nThe BLIP model is trained on a dataset of images with corresponding captions, and the goal is to learn a mapping between the image and the corresponding caption. In this case, the BLIP model has learned that the image of two motorcycles parked next to each other on the street is associated with the license number 4321234567.\\n\\nThe BLIP model can be used for a variety of applications, such as image captioning, visual question answering, and image-to-image translation. The BLIP model has the advantage of being able to generate captions for images that are not present in the training data, which makes it useful for generating captions for new images.\\n\\nThe BLIP model is based on a combination of convolutional neural networks (CNNs) and recurrent neural networks (RNNs), which allows it to learn both local and global features of an image. The CNNs are used to extract features from the image, and the RNNs are used to generate the caption. The BLIP model is trained end-to-end, which means that the model is trained as a whole, rather than training each component separately.\\n\\nThe BLIP model has been shown to be effective in generating accurate and informative captions for a variety of images, including objects, scenes, and actions. The BLIP model has also been used for visual question answering, where the model is asked to generate a caption for a given image in response to a question.\\n\\nIn summary, the BLIP model is a powerful tool for generating accurate and informative image captions. The BLIP model has the advantage of being able to generate captions for images that are not present in the training data, which makes it useful for generating captions for new images. The BLIP model is based on a combination of CNNs and RNNs, which allows it to learn both local and global features of an image.\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [19:24<06:09, 184.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': 'Based on the image caption (generated by BLIP model): there are two motorcycles parked next to each other on the street, Answer the question \\'Is this a motorcycle or bike?\\' \\n Answer:  Motorcycle\\n\\nExplanation:  Based on the image caption, we can determine that the vehicles in the image are motorcycles, rather than bicycles. The caption specifically mentions \"motorcycles\" and does not include any language that would suggest otherwise. Therefore, the answer to the question \\'Is this a motorcycle or bike?\\' is \\'Motorcycle\\'.'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [19:43<02:12, 132.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there are two motorcycles parked next to each other on the street, Answer the question 'What color is the bike?' \\n Answer:  The color of the bike is blue. \"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [20:07<00:00, 120.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there are two motorcycles parked next to each other on the street, Answer the question 'What letter and 3 numbers are on the tag?' \\n Answer:  The letter is 'A' and the numbers are '345'.\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "for instance in tqdm(results):\n",
    "    # instance = results[indx]\n",
    "    img_id = instance['image_id']\n",
    "    question = instance['question']\n",
    "    file, caption = lookup[img_id]\n",
    "    prompt = f\"Based on the image caption (generated by BLIP model): {caption}, Answer the question '{question}' \\n Answer: \"\n",
    "    gen_text = llamaPipe(prompt)\n",
    "    print(f\"Generated text: {gen_text}\")\n",
    "    answers.append(gen_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:09<01:23,  9.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there is a dog that is laying on a rack with shoes. Answer the question: 'What color are the gym shoes?'\\n Answer: ���\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:19<01:17,  9.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there is a dog that is laying on a rack with shoes. Answer the question: 'Is there a red sandal here?'\\n Answer:  Yes, there\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:28<01:07,  9.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there is a dog that is laying on a rack with shoes. Answer the question: 'What color is the flip flop?'\\n Answer: ���\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:38<00:58,  9.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): araffe dog laying on the ground next to a bike on a city street. Answer the question: 'Does this dog have a collar?'\\n Answer:  Yes \"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:48<00:48,  9.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): araffe dog laying on the ground next to a bike on a city street. Answer the question: 'Where is the dog laying?'\\n Answer:  The dog is\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:58<00:39,  9.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): araffe dog laying on the ground next to a bike on a city street. Answer the question: 'What is the dog doing?'\\n Answer:  The dog is\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [01:07<00:28,  9.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there are two motorcycles parked next to each other on the street. Answer the question: 'What is the license number?'\\n Answer: \\nThe B\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [01:17<00:19,  9.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there are two motorcycles parked next to each other on the street. Answer the question: 'Is this a motorcycle or bike?'\\n Answer:  Yes, this\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [01:26<00:09,  9.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there are two motorcycles parked next to each other on the street. Answer the question: 'What color is the bike?'\\n Answer:  Red \\n\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:36<00:00,  9.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there are two motorcycles parked next to each other on the street. Answer the question: 'What letter and 3 numbers are on the tag?'\\n Answer:  The letter is\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Restriction by configuration to restrict to 3 tokens without prompt\n",
    "generation_cfg_answers = []\n",
    "for instance in tqdm(results):\n",
    "    img_id = instance['image_id']\n",
    "    question = instance['question']\n",
    "    file, caption = lookup[img_id]\n",
    "    prompt = f\"Based on the image caption (generated by BLIP model): {caption}. Answer the question: '{question}'\\n Answer: \"\n",
    "    gen_text = llamaPipe(prompt, max_new_tokens = 3)\n",
    "    print(f\"Generated text: {gen_text}\")\n",
    "    generation_cfg_answers.append(gen_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:10<01:35, 10.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there is a dog that is laying on a rack with shoes. Answer in a single word the question: 'What color are the gym shoes?'\\n Answer: ���\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:22<01:30, 11.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there is a dog that is laying on a rack with shoes. Answer in a single word the question: 'Is there a red sandal here?'\\n Answer:  Yes \"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:32<01:14, 10.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there is a dog that is laying on a rack with shoes. Answer in a single word the question: 'What color is the flip flop?'\\n Answer:  Brown\\n\\n\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:42<01:03, 10.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): araffe dog laying on the ground next to a bike on a city street. Answer in a single word the question: 'Does this dog have a collar?'\\n Answer:  Yes \"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:52<00:52, 10.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): araffe dog laying on the ground next to a bike on a city street. Answer in a single word the question: 'Where is the dog laying?'\\n Answer:  ground \"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [01:03<00:41, 10.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): araffe dog laying on the ground next to a bike on a city street. Answer in a single word the question: 'What is the dog doing?'\\n Answer:  laying \"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [01:13<00:30, 10.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there are two motorcycles parked next to each other on the street. Answer in a single word the question: 'What is the license number?'\\n Answer: 739\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [01:23<00:20, 10.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there are two motorcycles parked next to each other on the street. Answer in a single word the question: 'Is this a motorcycle or bike?'\\n Answer:  Motorcycle\\n\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [01:32<00:10, 10.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there are two motorcycles parked next to each other on the street. Answer in a single word the question: 'What color is the bike?'\\n Answer:  Red\\n\\n\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:43<00:00, 10.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there are two motorcycles parked next to each other on the street. Answer in a single word the question: 'What letter and 3 numbers are on the tag?'\\n Answer: 234\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Restriction by configuration to restrict to 3 tokens with restriction to single word in prompt\n",
    "generation_cfg_prompt_restriction_answers = []\n",
    "for instance in tqdm(results):\n",
    "    img_id = instance['image_id']\n",
    "    question = instance['question']\n",
    "    file, caption = lookup[img_id]\n",
    "    prompt = f\"Based on the image caption (generated by BLIP model): {caption}. Answer in a single word the question: '{question}'\\n Answer: \"\n",
    "    gen_text = llamaPipe(prompt, max_new_tokens = 3)\n",
    "    print(f\"Generated text: {gen_text}\")\n",
    "    generation_cfg_prompt_restriction_answers.append(gen_text)\n",
    "# llamaPipe(\"What are the benefits of being vegetarian? Write a brief summary around the advantages and disadvantages of being vegetarian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:07<01:08,  7.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there is a dog that is laying on a rack with shoes. Answer the question: 'What color are the gym shoes?'\\n Answer: �\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:14<00:58,  7.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there is a dog that is laying on a rack with shoes. Answer the question: 'Is there a red sandal here?'\\n Answer:  Yes\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:21<00:49,  7.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there is a dog that is laying on a rack with shoes. Answer the question: 'What color is the flip flop?'\\n Answer: �\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:29<00:43,  7.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): araffe dog laying on the ground next to a bike on a city street. Answer the question: 'Does this dog have a collar?'\\n Answer:  Yes\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:36<00:36,  7.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): araffe dog laying on the ground next to a bike on a city street. Answer the question: 'Where is the dog laying?'\\n Answer:  The\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:44<00:29,  7.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): araffe dog laying on the ground next to a bike on a city street. Answer the question: 'What is the dog doing?'\\n Answer:  '\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:51<00:22,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there are two motorcycles parked next to each other on the street. Answer the question: 'What is the license number?'\\n Answer: 0\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:59<00:14,  7.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there are two motorcycles parked next to each other on the street. Answer the question: 'Is this a motorcycle or bike?'\\n Answer:  Motor\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [01:06<00:07,  7.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there are two motorcycles parked next to each other on the street. Answer the question: 'What color is the bike?'\\n Answer:  The\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:14<00:00,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there are two motorcycles parked next to each other on the street. Answer the question: 'What letter and 3 numbers are on the tag?'\\n Answer: \\n\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:17<02:40, 17.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there is a dog that is laying on a rack with shoes. \\n In plain simple English language without any emoticons or icons or font colors or punctuation marks. I strongly state do not repeat the question, prompt used, disclaimer, explanantion or anything apart from answer, that is just provide the answer in a single word in lowercase, the question: 'What color are the gym shoes?'. Remember if you are unable to answer the question based on the caption provided by BLIP, mention 'NA'.\\nAnswer:  shoes\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:34<02:18, 17.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there is a dog that is laying on a rack with shoes. \\n In plain simple English language without any emoticons or icons or font colors or punctuation marks. I strongly state do not repeat the question, prompt used, disclaimer, explanantion or anything apart from answer, that is just provide the answer in a single word in lowercase, the question: 'Is there a red sandal here?'. Remember if you are unable to answer the question based on the caption provided by BLIP, mention 'NA'.\\nAnswer:  dog\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:52<02:01, 17.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there is a dog that is laying on a rack with shoes. \\n In plain simple English language without any emoticons or icons or font colors or punctuation marks. I strongly state do not repeat the question, prompt used, disclaimer, explanantion or anything apart from answer, that is just provide the answer in a single word in lowercase, the question: 'What color is the flip flop?'. Remember if you are unable to answer the question based on the caption provided by BLIP, mention 'NA'.\\nAnswer: ���\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [01:09<01:43, 17.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): araffe dog laying on the ground next to a bike on a city street. \\n In plain simple English language without any emoticons or icons or font colors or punctuation marks. I strongly state do not repeat the question, prompt used, disclaimer, explanantion or anything apart from answer, that is just provide the answer in a single word in lowercase, the question: 'Does this dog have a collar?'. Remember if you are unable to answer the question based on the caption provided by BLIP, mention 'NA'.\\nAnswer:  dog\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:27<01:27, 17.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): araffe dog laying on the ground next to a bike on a city street. \\n In plain simple English language without any emoticons or icons or font colors or punctuation marks. I strongly state do not repeat the question, prompt used, disclaimer, explanantion or anything apart from answer, that is just provide the answer in a single word in lowercase, the question: 'Where is the dog laying?'. Remember if you are unable to answer the question based on the caption provided by BLIP, mention 'NA'.\\nAnswer:  dog \"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [01:44<01:09, 17.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): araffe dog laying on the ground next to a bike on a city street. \\n In plain simple English language without any emoticons or icons or font colors or punctuation marks. I strongly state do not repeat the question, prompt used, disclaimer, explanantion or anything apart from answer, that is just provide the answer in a single word in lowercase, the question: 'What is the dog doing?'. Remember if you are unable to answer the question based on the caption provided by BLIP, mention 'NA'.\\nAnswer:  dog\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [02:01<00:51, 17.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there are two motorcycles parked next to each other on the street. \\n In plain simple English language without any emoticons or icons or font colors or punctuation marks. I strongly state do not repeat the question, prompt used, disclaimer, explanantion or anything apart from answer, that is just provide the answer in a single word in lowercase, the question: 'What is the license number?'. Remember if you are unable to answer the question based on the caption provided by BLIP, mention 'NA'.\\nAnswer: 257\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [02:19<00:35, 17.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there are two motorcycles parked next to each other on the street. \\n In plain simple English language without any emoticons or icons or font colors or punctuation marks. I strongly state do not repeat the question, prompt used, disclaimer, explanantion or anything apart from answer, that is just provide the answer in a single word in lowercase, the question: 'Is this a motorcycle or bike?'. Remember if you are unable to answer the question based on the caption provided by BLIP, mention 'NA'.\\nAnswer:  motorcycle\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [02:35<00:17, 17.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there are two motorcycles parked next to each other on the street. \\n In plain simple English language without any emoticons or icons or font colors or punctuation marks. I strongly state do not repeat the question, prompt used, disclaimer, explanantion or anything apart from answer, that is just provide the answer in a single word in lowercase, the question: 'What color is the bike?'. Remember if you are unable to answer the question based on the caption provided by BLIP, mention 'NA'.\\nAnswer:  blue\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:52<00:00, 17.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there are two motorcycles parked next to each other on the street. \\n In plain simple English language without any emoticons or icons or font colors or punctuation marks. I strongly state do not repeat the question, prompt used, disclaimer, explanantion or anything apart from answer, that is just provide the answer in a single word in lowercase, the question: 'What letter and 3 numbers are on the tag?'. Remember if you are unable to answer the question based on the caption provided by BLIP, mention 'NA'.\\nAnswer: 2\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#### Configuration to generate only 3 tokens at max and Enhanced prompt\n",
    "prompt_with_generation_cfg_answers = []\n",
    "for instance in tqdm(results):\n",
    "    img_id = instance['image_id']\n",
    "    question = instance['question']\n",
    "    file, caption = lookup[img_id]\n",
    "    prompt = f\"Based on the image caption (generated by BLIP model): {caption}. \\n In plain simple English language without any emoticons or icons or font colors or punctuation marks. I strongly state do not repeat the question, prompt used, disclaimer, explanantion or anything apart from answer, that is just provide the answer in a single word in lowercase, the question: '{question}'. Remember if you are unable to answer the question based on the caption provided by BLIP, mention 'NA'.\\nAnswer: \"\n",
    "    gen_text = llamaPipe(prompt, max_new_tokens = 3)\n",
    "    print(f\"Generated text: {gen_text}\")\n",
    "    prompt_with_generation_cfg_answers.append(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': \"Based on the image caption (generated by BLIP model): there is a dog that is laying on a rack with shoes. \\n In plain simple English language without any emoticons or icons or font colors or punctuation marks. I strongly state do not repeat the question, prompt used, disclaimer, explanantion or anything apart from answer, that is just provide the answer in a single word in lowercase, the question: 'What color are the gym shoes?'. Remember if you are unable to answer the question based on the caption provided by BLIP, mention 'NA'.\\nAnswer:  shoes\"}],\n",
       " [{'generated_text': \"Based on the image caption (generated by BLIP model): there is a dog that is laying on a rack with shoes. \\n In plain simple English language without any emoticons or icons or font colors or punctuation marks. I strongly state do not repeat the question, prompt used, disclaimer, explanantion or anything apart from answer, that is just provide the answer in a single word in lowercase, the question: 'Is there a red sandal here?'. Remember if you are unable to answer the question based on the caption provided by BLIP, mention 'NA'.\\nAnswer:  dog\"}],\n",
       " [{'generated_text': \"Based on the image caption (generated by BLIP model): there is a dog that is laying on a rack with shoes. \\n In plain simple English language without any emoticons or icons or font colors or punctuation marks. I strongly state do not repeat the question, prompt used, disclaimer, explanantion or anything apart from answer, that is just provide the answer in a single word in lowercase, the question: 'What color is the flip flop?'. Remember if you are unable to answer the question based on the caption provided by BLIP, mention 'NA'.\\nAnswer: ���\"}],\n",
       " [{'generated_text': \"Based on the image caption (generated by BLIP model): araffe dog laying on the ground next to a bike on a city street. \\n In plain simple English language without any emoticons or icons or font colors or punctuation marks. I strongly state do not repeat the question, prompt used, disclaimer, explanantion or anything apart from answer, that is just provide the answer in a single word in lowercase, the question: 'Does this dog have a collar?'. Remember if you are unable to answer the question based on the caption provided by BLIP, mention 'NA'.\\nAnswer:  dog\"}],\n",
       " [{'generated_text': \"Based on the image caption (generated by BLIP model): araffe dog laying on the ground next to a bike on a city street. \\n In plain simple English language without any emoticons or icons or font colors or punctuation marks. I strongly state do not repeat the question, prompt used, disclaimer, explanantion or anything apart from answer, that is just provide the answer in a single word in lowercase, the question: 'Where is the dog laying?'. Remember if you are unable to answer the question based on the caption provided by BLIP, mention 'NA'.\\nAnswer:  dog \"}],\n",
       " [{'generated_text': \"Based on the image caption (generated by BLIP model): araffe dog laying on the ground next to a bike on a city street. \\n In plain simple English language without any emoticons or icons or font colors or punctuation marks. I strongly state do not repeat the question, prompt used, disclaimer, explanantion or anything apart from answer, that is just provide the answer in a single word in lowercase, the question: 'What is the dog doing?'. Remember if you are unable to answer the question based on the caption provided by BLIP, mention 'NA'.\\nAnswer:  dog\"}],\n",
       " [{'generated_text': \"Based on the image caption (generated by BLIP model): there are two motorcycles parked next to each other on the street. \\n In plain simple English language without any emoticons or icons or font colors or punctuation marks. I strongly state do not repeat the question, prompt used, disclaimer, explanantion or anything apart from answer, that is just provide the answer in a single word in lowercase, the question: 'What is the license number?'. Remember if you are unable to answer the question based on the caption provided by BLIP, mention 'NA'.\\nAnswer: 257\"}],\n",
       " [{'generated_text': \"Based on the image caption (generated by BLIP model): there are two motorcycles parked next to each other on the street. \\n In plain simple English language without any emoticons or icons or font colors or punctuation marks. I strongly state do not repeat the question, prompt used, disclaimer, explanantion or anything apart from answer, that is just provide the answer in a single word in lowercase, the question: 'Is this a motorcycle or bike?'. Remember if you are unable to answer the question based on the caption provided by BLIP, mention 'NA'.\\nAnswer:  motorcycle\"}],\n",
       " [{'generated_text': \"Based on the image caption (generated by BLIP model): there are two motorcycles parked next to each other on the street. \\n In plain simple English language without any emoticons or icons or font colors or punctuation marks. I strongly state do not repeat the question, prompt used, disclaimer, explanantion or anything apart from answer, that is just provide the answer in a single word in lowercase, the question: 'What color is the bike?'. Remember if you are unable to answer the question based on the caption provided by BLIP, mention 'NA'.\\nAnswer:  blue\"}],\n",
       " [{'generated_text': \"Based on the image caption (generated by BLIP model): there are two motorcycles parked next to each other on the street. \\n In plain simple English language without any emoticons or icons or font colors or punctuation marks. I strongly state do not repeat the question, prompt used, disclaimer, explanantion or anything apart from answer, that is just provide the answer in a single word in lowercase, the question: 'What letter and 3 numbers are on the tag?'. Remember if you are unable to answer the question based on the caption provided by BLIP, mention 'NA'.\\nAnswer: 2\"}]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_with_generation_cfg_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ToDo \n",
    "### Write a function to evaluate the model performance as accuracy or the model\n",
    "\n",
    "# Experiment with prompt to provide a single word answer. Done\n",
    "\n",
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_results(ground_truths: list[str], generated_texts: list[dict]):\n",
    "    \"\"\" This function provides the results based on the two lists \"\"\"\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
