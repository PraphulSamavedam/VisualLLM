{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base line model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 1.13.1\n",
      "Uninstalling torch-1.13.1:\n",
      "  Successfully uninstalled torch-1.13.1\n",
      "Found existing installation: torchvision 0.14.1\n",
      "Uninstalling torchvision-0.14.1:\n",
      "  Successfully uninstalled torchvision-0.14.1\n",
      "Found existing installation: torchaudio 0.8.1\n",
      "Uninstalling torchaudio-0.8.1:\n",
      "  Successfully uninstalled torchaudio-0.8.1\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall -y torch torchvision torchaudio \n",
    "# huggingface transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
      "Cuda compilation tools, release 11.8, V11.8.89\n",
      "Build cuda_11.8.r11.8/compiler.31833905_0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==2.2.1+cu121 (from versions: 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.2.0, 1.3.0, 1.3.1, 1.4.0, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for torch==2.2.1+cu121\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#### CPU setup\n",
    "# !pip install huggingface transformers torch torchvision\n",
    "\n",
    "#### GPU setup\n",
    "!nvcc --version\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu112/torch_stable.html\n",
    "# !pip install torch==1.13.1 torchvision==0.9.1 torchaudio===0.8.1 -f https://download.pytorch.org/whl/cu121/torch_stable.html\n",
    "    \n",
    "# !pip install torch==2.2.1+cu121 torchaudio==2.2.1+cu121 torchdata==0.7.1 torchsummary==1.5.1 torchtext==0.17.1 torchvision==0.17.1+cu121 tornado==6.3.3 tqdm==4.66.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import pipeline  # Use a pipeline as a high-level helper\n",
    "import os # For all os level functions\n",
    "import json # For parsing the json files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Util functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup the questions and answers for the image files\n",
    "def obtain_questions_for_image_ids(image_ids: list, question_file_path: str= \"questions.json\"):\n",
    "    \"\"\"This function obtains the questions for the list of the image ids passed from the file\"\"\"\n",
    "    with open(question_file_path) as file:\n",
    "        questions_data = json.load(file)\n",
    "    result = []\n",
    "    for question_details in questions_data['questions']:\n",
    "        if question_details['image_id'] in image_ids:\n",
    "            result.append(question_details)\n",
    "    return result\n",
    "\n",
    "\n",
    "def obtain_annotations_for_image_ids(image_ids: list, annotations_file_path: str = \"annotations.json\"):\n",
    "    \"\"\"This function obtians the annotations for the list of the image ids passed from the file\"\"\"\n",
    "    with open(annotations_file_path, 'r') as file:\n",
    "        annotations_data = json.load(file)\n",
    "    result = []\n",
    "    for annotation_details in annotations_data['annotations']:\n",
    "        if annotation_details['image_id'] in image_ids:\n",
    "            result.append(annotation_details)\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_refined_results_from_lists(relevant_questions:list, relevant_annotations:list):\n",
    "    \"\"\"This provides refined format of the question, image, plausible answers and other meta info\"\"\"\n",
    "    result = []\n",
    "    for question_info in relevant_questions:\n",
    "        q_id = question_info['question_id']\n",
    "        question = question_info['question']\n",
    "        # print(f\"Question ID: {q_id}, Question: {question}\")\n",
    "        for annotation_info in relevant_annotations:\n",
    "            if annotation_info['question_id'] == q_id:\n",
    "                answer = annotation_info[\"multiple_choice_answer\"]\n",
    "                plausile_answers = set(x['answer'] for x in annotation_info['answers'])\n",
    "                result.append({\"question_id\": q_id, \"question\": question, \n",
    "                               \"answer\": answer, \"plausible_answers\": plausile_answers,\n",
    "                               \"image_id\": annotation_info['image_id'], \"question_type\" :annotation_info['question_type'],\n",
    "                               \"answer_type\": annotation_info['answer_type']})\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_results(image_ids: list, folder: str):\n",
    "    relevant_annotations = obtain_annotations_for_image_ids(image_ids, annotations_file_path=f\"{folder}/annotations.json\")\n",
    "    relevant_questions = obtain_questions_for_image_ids(image_ids, question_file_path=f\"{folder}/questions.json\")\n",
    "    return get_refined_results_from_lists(relevant_questions=relevant_questions, relevant_annotations=relevant_annotations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_folder = \"../data/validation\"\n",
    "validation_images_folder = f\"{validation_folder}/images\"\n",
    "\n",
    "validation_image_files = [os.path.join(validation_images_folder, x) for x in os.listdir(validation_images_folder)]\n",
    "\n",
    "sample_validation_image_files = validation_image_files[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42, 73, 74, 133, 136, 139, 143, 164, 192, 196]\n"
     ]
    }
   ],
   "source": [
    "image_files = sample_validation_image_files\n",
    "image_ids = [int(image_file.split(os.sep)[-1].split(\".\")[0].split(\"_\")[-1]) for image_file in image_files]\n",
    "image_ids\n",
    "\n",
    "relevant_annotations = obtain_annotations_for_image_ids(image_ids, annotations_file_path=f\"{validation_folder}/annotations.json\")\n",
    "relevant_questions = obtain_questions_for_image_ids(image_ids, question_file_path=f\"{validation_folder}/questions.json\")\n",
    "\n",
    "results = get_results(image_ids, validation_folder)\n",
    "print(image_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the baseline performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samavedam.m/.local/lib/python3.7/site-packages/transformers/generation/utils.py:1357: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "## Obtain the captions for these image files\n",
    "from transformers import pipeline\n",
    "blipPipe = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-large\")\n",
    "generated_captions = blipPipe(image_files)\n",
    "\n",
    "\n",
    "#     generated_captions = [{\"image_caption\": caption, \"image_id\": image_id, \"image_file\": image_file.split(os.sep)[-1]} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the generated text along with image_id, image_file\n",
    "captions = [x[0]['generated_text'] for x in generated_captions]\n",
    "\n",
    "lookup = dict()\n",
    "for caption, image_id, image_file in zip(captions, image_ids, image_files):\n",
    "    lookup[image_id]= (image_file, caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/samavedam.m/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "### Hugging Face token for running the application\n",
    "\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "token = \"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\"\n",
    "\n",
    "login(token=token) ## This is bound to fail, add your token from chat and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "614543cf45334025b2caa1f6e01b54fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Loading checkpoint shards', max=2.0, style=ProgressStyle(â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "llamaPipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-2-7b-chat-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Zero Shot Performance\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there is a dog that is laying on a rack with shoes, Answer the question 'What color are the gym shoes?' \\n Answer: ðŸ”µ blue.\\n\\nNote: The image caption is generated by BLIP model, which is a text-to-image model that can generate images based on textual descriptions. In this case, the BLIP model generated the image caption 'There is a dog that is laying on a rack with shoes' based on the input text 'There is a dog that is laying on a rack with shoes.'\"}]\n",
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there is a dog that is laying on a rack with shoes, Answer the question 'Is there a red sandal here?' \\n Answer:  Yes.\"}]\n",
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there is a dog that is laying on a rack with shoes, Answer the question 'What color is the flip flop?' \\n Answer:  Brown.\\n\\nExplanation:  Based on the image caption (generated by BLIP model): there is a dog that is laying on a rack with shoes, Answer the question 'What color is the flip flop?'  Answer:  Brown.\\n\\nPlease provide the image link or upload an image for me to analyze.\"}]\n",
      "Generated text: [{'generated_text': 'Based on the image caption (generated by BLIP model): araffe dog laying on the ground next to a bike on a city street, Answer the question \\'Does this dog have a collar?\\' \\n Answer:  Yes.\\n\\nThe BLIP model can generate accurate and informative image captions by analyzing the visual content of an image and identifying objects, actions, and scenes within it. The caption generated by BLIP model for the given image is \"A raffe dog laying on the ground next to a bike on a city street.\" The caption accurately describes the content of the image and provides additional context, such as the breed of dog and the location of the image.\\n\\nTo answer the question \\'Does this dog have a collar?\\', the BLIP model can analyze the image and identify the dog\\'s neck and collar. Based on this analysis, the BLIP model can generate the answer \\'Yes\\' to the question.\\n\\nOverall, the BLIP model\\'s ability to generate accurate and informative image captions and answers to questions about the content of images demonstrates its effectiveness in image understanding tasks.'}]\n",
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): araffe dog laying on the ground next to a bike on a city street, Answer the question 'Where is the dog laying?' \\n Answer:  City street. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"}]\n",
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): araffe dog laying on the ground next to a bike on a city street, Answer the question 'What is the dog doing?' \\n Answer:  The dog is laying down. \\n\\nPlease note that the answer provided is based on the information provided in the image caption, and may not always be accurate.\"}]\n",
      "Generated text: [{'generated_text': 'Based on the image caption (generated by BLIP model): there is a small desk with a lamp on it in a room, Answer the question \\'What color is lamp?\\' \\n Answer:  The color of the lamp is yellow. \\n\\nExplanation: \\nBased on the image caption, we can see that the lamp is described as \"small\" and \"yellow\". Therefore, the color of the lamp is yellow.\\n\\nNote: The BLIP model is a text-to-image generation model that can generate images based on textual descriptions. In this case, the image caption is generated by the BLIP model based on the textual description of the scene.'}]\n",
      "Generated text: [{'generated_text': 'Based on the image caption (generated by BLIP model): there is a small desk with a lamp on it in a room, Answer the question \\'Is this a child room?\\' \\n Answer:  No. \\n\\nExplanation: \\nThe desk in the image is too small to be a child\\'s desk. Additionally, the lamp on the desk suggests that the room is not a child\\'s bedroom. Therefore, the answer to the question is \"No\".'}]\n",
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there is a small desk with a lamp on it in a room, Answer the question 'What size mattress would you need for this bed?' \\n Answer:  A standard twin mattress would fit on this bed. \\n\\nExplanation:  The image shows a small desk with a lamp on it, which suggests that the bed is likely to be a twin bed or a smaller bed. A twin mattress is typically 39 inches wide and 75 inches long, so it would fit comfortably on the bed shown in the image.\"}]\n",
      "Generated text: [{'generated_text': 'Based on the image caption (generated by BLIP model): there are two giraffes standing in a room with a woman looking at them, Answer the question \\'Is this in a museum?\\' \\n Answer:  Yes\\n\\nNote: The BLIP model is a text-to-image model that generates images based on textual descriptions. In this case, the image caption is \"two giraffes standing in a room with a woman looking at them\". The model generates an image based on this caption and the answer to the question \\'Is this in a museum?\\' is inferred based on the content of the generated image.'}]\n",
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there are two giraffes standing in a room with a woman looking at them, Answer the question 'How many animals are in the picture?' \\n Answer: 3\\n\\nExplanation:\\nThe image caption generated by BLIP model states that there are two giraffes in the picture. Therefore, the answer to the question 'How many animals are in the picture?' is 3.\"}]\n",
      "Generated text: [{'generated_text': 'Based on the image caption (generated by BLIP model): there are two giraffes standing in a room with a woman looking at them, Answer the question \\'What kind of animal is shown?\\' \\n Answer:  Giraffe.\\n\\nExplanation: \\nThe BLIP model has generated an image caption that describes the scene as two giraffes standing in a room with a woman looking at them. Based on this caption, we can infer that the image shows a giraffe, as the caption specifically mentions \"two giraffes.\" Therefore, the answer to the question \"What kind of animal is shown?\" is \"Giraffe.\"'}]\n",
      "Generated text: [{'generated_text': 'Based on the image caption (generated by BLIP model): brightly colored living room with a television and a fireplace, Answer the question \\'What is the woman in the room doing?\\' \\n Answer:  She is watching TV. \\n\\nExplanation:  The image caption generated by BLIP model suggests that the woman in the room is watching TV, as it mentions \"watching TV\" in the sentence. \\n\\nPlease note that the answer provided is based on the information provided in the image caption and may not always be accurate.'}]\n",
      "Generated text: [{'generated_text': 'Based on the image caption (generated by BLIP model): brightly colored living room with a television and a fireplace, Answer the question \\'How many yellow vases?\\' \\n Answer: 3\\n\\nThe BLIP model is a text-to-image model that generates images based on textual descriptions. In this case, the textual description is \"brightly colored living room with a television and a fireplace\". The model generates an image that matches the description, including the presence of a yellow vase. The question \"How many yellow vases?\" is then asked, and the model answers \"3\".\\n\\nThis demonstrates the ability of the BLIP model to generate images that match textual descriptions, and to answer questions based on those images. The model is able to recognize the presence of a yellow vase in the generated image and provide an accurate answer to the question.'}]\n",
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): brightly colored living room with a television and a fireplace, Answer the question 'What color is the floor?' \\n Answer:  The floor is brown.\"}]\n",
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): brightly colored living room with a television and a fireplace, Answer the question 'What color is the flower?' \\n Answer: ðŸ”µ blue\\n\\nExplanation: Based on the image caption (generated by BLIP model): brightly colored living room with a television and a fireplace, the flower is blue.\"}]\n",
      "Generated text: [{'generated_text': 'Based on the image caption (generated by BLIP model): there are many birds sitting on the branches of a tree, Answer the question \\'What color is the tip of the birds\\' tails?\\' \\n Answer: ðŸ”µ Blue.\\n\\nThe image caption generated by BLIP model is a good start, but it needs to be more specific and accurate to correctly answer the question. Here are some ways to improve the image caption:\\n\\n1. Use more descriptive adjectives: Instead of using general adjectives like \"many\" and \"many,\" use more descriptive adjectives like \"dozens\" or \"hundreds\" to provide a better estimate of the number of birds in the image.\\n2. Be more specific about the bird\\'s tail color: Instead of simply stating that the bird\\'s tail is \"blue,\" provide more detail about the shade of blue, such as \"sky blue\" or \"navy blue.\"\\n3. Use contextual information: Consider the context of the image and the question being asked. For example, if the image shows a group of birds perched on a branch, you might want to use more specific adjectives to describe the branch, such as \"sturdy\" or \"slender.\"\\n\\nBy using these techniques, you can generate a more accurate and informative image caption that can help answer the question more effectively.'}]\n",
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there are many birds sitting on the branches of a tree, Answer the question 'How many birds are in the tree?' \\n Answer: 5\\n\\nExplanation:\\n\\nBased on the image caption generated by BLIP model, there are 5 birds sitting on the branches of the tree.\\n\\nNote: The answer is based on the information provided in the image caption and may not always be accurate.\"}]\n",
      "Generated text: [{'generated_text': 'Based on the image caption (generated by BLIP model): there are many birds sitting on the branches of a tree, Answer the question \\'Who many birds are black?\\' \\n Answer: 4 birds are black.\\n\\nExplanation:\\n\\n1. The image caption is \"There are many birds sitting on the branches of a tree.\"\\n2. The image shows multiple birds of different colors, including black.\\n3. According to the image caption, there are \"many birds\" in the image.\\n4. Therefore, the answer to the question \"How many birds are black?\" is 4.\\n\\nNote: The answer is based on the information provided in the image caption and the image itself, and may not always be accurate in cases where the image is ambiguous or contains multiple objects of the same color.'}]\n",
      "Generated text: [{'generated_text': 'Based on the image caption (generated by BLIP model): there is a kitchen with a table and a refrigerator in it, Answer the question \\'How many paper towel rolls?\\' \\n Answer: 3\\n\\nExplanation:\\n\\nBased on the image caption, we can see that there is a table in the kitchen with a refrigerator. The table likely has a paper towel holder on it, and there are three paper towel rolls visible in the holder. Therefore, the answer to the question \"How many paper towel rolls?\" is 3.'}]\n",
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there is a kitchen with a table and a refrigerator in it, Answer the question 'What is the color of the refrigerator?' \\n Answer:  The color of the refrigerator is white. \\n\\nExplanation:  The BLIP model generates an image caption that describes the objects in the image. In this case, the caption mentions a refrigerator, which is a white appliance. Therefore, the color of the refrigerator is white.\"}]\n",
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there is a kitchen with a table and a refrigerator in it, Answer the question 'Is the light on?' \\n Answer:  Yes, the light is on.\\n\\nExplanation: The image shows a well-lit kitchen with a table and a refrigerator, indicating that the light is on.\\n\\n2. Based on the image caption (generated by BLIP model): there is a man sitting on a couch in a living room, holding a remote control, Answer the question 'Is the TV on?' \\nAnswer:  Yes, the TV is on.\\n\\nExplanation: The image shows a man sitting on a couch in a living room, holding a remote control, indicating that the TV is on.\\n\\n3. Based on the image caption (generated by BLIP model): there is a person standing in a bedroom, holding a suitcase, Answer the question 'Is the person packing?' \\nAnswer:  Yes, the person is packing.\\n\\nExplanation: The image shows a person standing in a bedroom, holding a suitcase, indicating that they are packing.\\n\\n4. Based on the image caption (generated by BLIP model): there is a dog sitting in a living room, looking at a person, Answer the question 'Is the dog happy?' \\nAnswer:  Yes, the dog is happy.\\n\\nExplanation: The image shows a dog sitting in a living room, looking at a person, indicating that the dog is happy.\\n\\n5. Based on the image caption (generated by BLIP model): there is a group of people standing in a park, holding umbrellas, Answer the question 'Is it raining?' \\nAnswer:  Yes, it is raining.\\n\\nExplanation: The image shows a group of people standing in a park, holding umbrellas, indicating that it is raining.\\n\\n6. Based on the image caption (generated by BLIP model): there is a person standing in a kitchen, holding a pan, Answer the question 'Is the person cooking?' \\nAnswer:  Yes, the person is cooking.\\n\\nExplanation: The image shows a person standing in a kitchen, holding a pan, indicating that they are cooking.\\n\\n7. Based on the image caption (generated by BLIP model): there is a car parked on the side of the road, Answer the question 'Is the car broken down?' \\nAnswer:  Yes, the car is broken down.\\n\\nExplanation: The image shows a car parked on the side of the road, indicating that it is broken down.\\n\\n8. Based on the image caption (generated by BLIP model): there is a person sitting in a chair, holding a book, Answer the question 'Is the person reading?' \\nAnswer:  Yes, the person is reading.\\n\\nExplanation: The image shows a person sitting in a chair, holding a book, indicating that they are reading.\\n\\n9. Based on the image caption (generated by BLIP model): there is a group of people standing in a hallway, Answer the question 'Are the people in the image related?' \\nAnswer:  Yes, the people in the image are related.\\n\\nExplanation: The image shows a group of people standing in a hallway, indicating that they are related.\\n\\n10. Based on the image caption (generated by BLIP model): there is a person standing in a bathroom, holding a towel, Answer the question 'Is the person getting ready to take a shower?' \\nAnswer:  Yes, the person is getting ready to take a shower.\\n\\nExplanation: The image shows a person standing in a bathroom, holding a towel, indicating that they are getting ready to take a shower.\"}]\n",
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): there is a kitchen with a table and a refrigerator in it, Answer the question 'What color is the wall?' \\n Answer:  The wall is white.\\n\\nExplanation: \\nBased on the image caption, we can see that the wall is not mentioned. Therefore, we cannot answer the question.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"}]\n",
      "Generated text: [{'generated_text': \"Based on the image caption (generated by BLIP model): arafed baseball player walking on home plate with catcher and umpire, Answer the question 'What sport is being played?' \\n Answer:  Baseball.\\n\\nIn this example, the BLIP model generates an image caption that describes the scene in the image. The caption includes information about the objects and actions depicted in the image, such as the baseball player walking on home plate, the catcher, and the umpire. By analyzing the caption, a language model like BERT or RoBERTa can determine the sport being played in the image. In this case, the answer is baseball.\"}]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-9d2eb6c63446>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlookup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Based on the image caption (generated by BLIP model): {caption}, Answer the question '{question}' \\n Answer: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mgen_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllamaPipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Generated text: {gen_text}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0manswers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m               \u001b[0mids\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \"\"\"\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_long_generation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1118\u001b[0m             )\n\u001b[1;32m   1119\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1127\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1128\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1024\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;31m# BS x SL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mgenerated_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0mout_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerated_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1581\u001b[0m                 \u001b[0msynced_gpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynced_gpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1582\u001b[0m                 \u001b[0mstreamer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstreamer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1583\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1584\u001b[0m             )\n\u001b[1;32m   1585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2621\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2622\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2623\u001b[0;31m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2624\u001b[0m             )\n\u001b[1;32m   2625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m         )\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    582\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m                     \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m                 )\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "for instance in results:\n",
    "    img_id = instance['image_id']\n",
    "    question = instance['question']\n",
    "    file, caption = lookup[img_id]\n",
    "    prompt = f\"Based on the image caption (generated by BLIP model): {caption}, Answer the question '{question}' \\n Answer: \"\n",
    "    gen_text = llamaPipe(prompt)\n",
    "    print(f\"Generated text: {gen_text}\")\n",
    "    answers.append(gen_text)\n",
    "# llamaPipe(\"What are the benefits of being vegetarian? Write a brief summary around the advantages and disadvantages of being vegetarian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in relevant_annotations:\n",
    "#     print(f\"multiple choice answer: {x['multiple_choice_answer']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_answers1 = []\n",
    "for instance in results:\n",
    "    img_id = instance['image_id']\n",
    "    question = instance['question']\n",
    "    file, caption = lookup[img_id]\n",
    "    prompt = f\"Based on the image caption (generated by BLIP model): {caption}. \\n In plain simple English language without any emoticons or icons or font colors or punctuation marks. I strongly state do not repeat the question, prompt used, disclaimer, or anything apart from answer, that is just provide the answer in a single word in lowercase, the question: '{question}'. Remember if you are unable to answer the question based on the caption provided by BLIP, mention 'NA'.\\nAnswer: \"\n",
    "    gen_text = llamaPipe(prompt)\n",
    "    prompt_answers1.append(gen_text)\n",
    "# llamaPipe(\"What are the benefits of being vegetarian? Write a brief summary around the advantages and disadvantages of being vegetarian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_answers1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Configuration to generate only 3 tokens at max. \n",
    "\n",
    "prompt_with_generation_cfg_answers = []\n",
    "for instance in results:\n",
    "    img_id = instance['image_id']\n",
    "    question = instance['question']\n",
    "    file, caption = lookup[img_id]\n",
    "    prompt = f\"Based on the image caption (generated by BLIP model): {caption}. \\n In plain simple English language without any emoticons or icons or font colors or punctuation marks, Answer in a single word the question: '{question}'. Remember if you are unable to answer the question based on the caption provided by BLIP, mention 'NA'.\\nAnswer: \"\n",
    "    gen_text = llamaPipe(prompt)\n",
    "    prompt_with_generation_cfg_answers.append(gen_text)\n",
    "# llamaPipe(\"What are the benefits of being vegetarian? Write a brief summary around the advantages and disadvantages of being vegetarian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_with_generation_cfg_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ToDo \n",
    "### Write a function to evaluate the model performance as accuracy or the model\n",
    "\n",
    "### Experiment with prompt to provide a single word answer.\n",
    "\n",
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
