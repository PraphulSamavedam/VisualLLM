{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base line model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall torch torchvision torchaudio huggingface transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CPU setup\n",
    "# !pip install huggingface transformers torch torchvision\n",
    "\n",
    "#### GPU setup\n",
    "# !nvcc --version\n",
    "# !pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import pipeline  # Use a pipeline as a high-level helper\n",
    "import os # For all os level functions\n",
    "import json # For parsing the json files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.7\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Util functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup the questions and answers for the image files\n",
    "def obtain_questions_for_image_ids(image_ids: list, question_file_path: str= \"questions.json\"):\n",
    "    \"\"\"This function obtains the questions for the list of the image ids passed from the file\"\"\"\n",
    "    with open(question_file_path) as file:\n",
    "        questions_data = json.load(file)\n",
    "    result = []\n",
    "    for question_details in questions_data['questions']:\n",
    "        if question_details['image_id'] in image_ids:\n",
    "            result.append(question_details)\n",
    "    return result\n",
    "\n",
    "\n",
    "def obtain_annotations_for_image_ids(image_ids: list, annotations_file_path: str = \"annotations.json\"):\n",
    "    \"\"\"This function obtians the annotations for the list of the image ids passed from the file\"\"\"\n",
    "    with open(annotations_file_path, 'r') as file:\n",
    "        annotations_data = json.load(file)\n",
    "    result = []\n",
    "    for annotation_details in annotations_data['annotations']:\n",
    "        if annotation_details['image_id'] in image_ids:\n",
    "            result.append(annotation_details)\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_refined_results_from_lists(relevant_questions:list, relevant_annotations:list):\n",
    "    \"\"\"This provides refined format of the question, image, plausible answers and other meta info\"\"\"\n",
    "    result = []\n",
    "    for question_info in relevant_questions:\n",
    "        q_id = question_info['question_id']\n",
    "        question = question_info['question']\n",
    "        # print(f\"Question ID: {q_id}, Question: {question}\")\n",
    "        for annotation_info in relevant_annotations:\n",
    "            if annotation_info['question_id'] == q_id:\n",
    "                answer = annotation_info[\"multiple_choice_answer\"]\n",
    "                plausile_answers = set(x['answer'] for x in annotation_info['answers'])\n",
    "                result.append({\"question_id\": q_id, \"question\": question, \n",
    "                               \"answer\": answer, \"plausible_answers\": plausile_answers,\n",
    "                               \"image_id\": annotation_info['image_id'], \"question_type\" :annotation_info['question_type'],\n",
    "                               \"answer_type\": annotation_info['answer_type']})\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_results(image_ids: list, folder: str):\n",
    "    relevant_annotations = obtain_annotations_for_image_ids(image_ids, annotations_file_path=f\"{folder}/annotations.json\")\n",
    "    relevant_questions = obtain_questions_for_image_ids(image_ids, question_file_path=f\"{folder}/questions.json\")\n",
    "    return get_refined_results_from_lists(relevant_questions=relevant_questions, relevant_annotations=relevant_annotations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_folder = \"../data/validation\"\n",
    "validation_images_folder = f\"{validation_folder}/images\"\n",
    "\n",
    "validation_image_files = [os.path.join(validation_images_folder, x) for x in os.listdir(validation_images_folder)]\n",
    "\n",
    "sample_validation_image_files = validation_image_files[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42, 73, 74, 133, 136, 139, 143, 164, 192, 196]\n"
     ]
    }
   ],
   "source": [
    "image_files = sample_validation_image_files\n",
    "image_ids = [int(image_file.split(os.sep)[-1].split(\".\")[0].split(\"_\")[-1]) for image_file in image_files]\n",
    "image_ids\n",
    "\n",
    "relevant_annotations = obtain_annotations_for_image_ids(image_ids, annotations_file_path=f\"{validation_folder}/annotations.json\")\n",
    "relevant_questions = obtain_questions_for_image_ids(image_ids, question_file_path=f\"{validation_folder}/questions.json\")\n",
    "\n",
    "results = get_results(image_ids, validation_folder)\n",
    "print(image_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the baseline performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samavedam.m/.local/lib/python3.7/site-packages/transformers/generation/utils.py:1357: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "## Obtain the captions for these image files\n",
    "from transformers import pipeline\n",
    "blipPipe = pipeline(\"image-to-text\", model=\"Salesforce/blip-image-captioning-large\")\n",
    "generated_captions = blipPipe(image_files)\n",
    "\n",
    "\n",
    "#     generated_captions = [{\"image_caption\": caption, \"image_id\": image_id, \"image_file\": image_file.split(os.sep)[-1]} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the generated text along with image_id, image_file\n",
    "captions = [x[0]['generated_text'] for x in generated_captions]\n",
    "\n",
    "lookup = dict()\n",
    "for caption, image_id, image_file in zip(captions, image_ids, image_files):\n",
    "    lookup[image_id]= (image_file, caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/samavedam.m/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "### Hugging Face token for running the application\n",
    "\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=token) ## This is bound to fail, add your token from chat and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f12d243bf94beb85bbf5ab24614a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading config.json', max=614.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0389860be61242bdb1634b0768cd72c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading (…)fetensors.index.json', max=26788.0, style=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64886fbea1c241b2af00d3687aa15a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading shards', max=2.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b5a4553ef648269aaf82d218dde046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading (…)of-00002.safetensors', max=9976576152.0, s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55df7de35135489a85978a99335939e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading (…)of-00002.safetensors', max=3500296424.0, s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7129099d4cc4e7581cb4ac3fa5fb087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Loading checkpoint shards', max=2.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f98e096aa7a34cf8bd8ed479940ea7d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading generation_config.json', max=188.0, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03a690da33f47cf8d18d0007a02e097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading tokenizer_config.json', max=1618.0, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94bf96b6b8f94dc981183ed86b860081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading tokenizer.model', max=499723.0, style=Progres…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b23c77b83764ecdbac0c77e732bd214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading tokenizer.json', max=1842767.0, style=Progres…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "678a207aafe14205ba43b23228570f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading (…)cial_tokens_map.json', max=414.0, style=Pr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "llamaPipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-2-7b-chat-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Zero Shot Performance\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "for instance in results:\n",
    "    img_id = instance['image_id']\n",
    "    question = instance['question']\n",
    "    file, caption = lookup[img_id]\n",
    "    prompt = f\"Based on the image caption (generated by BLIP model): {caption}, Answer the question '{question}' \\n Answer: \"\n",
    "    gen_text = llamaPipe(prompt)\n",
    "    answers.append(gen_text)\n",
    "# llamaPipe(\"What are the benefits of being vegetarian? Write a brief summary around the advantages and disadvantages of being vegetarian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in relevant_annotations:\n",
    "#     print(f\"multiple choice answer: {x['multiple_choice_answer']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_answers1 = []\n",
    "for instance in results:\n",
    "    img_id = instance['image_id']\n",
    "    question = instance['question']\n",
    "    file, caption = lookup[img_id]\n",
    "    prompt = f\"Based on the image caption (generated by BLIP model): {caption}. \\n In plain simple English language without any emoticons or icons or font colors or punctuation marks, Answer in a single word the question: '{question}'. Remember if you are unable to answer the question based on the caption provided by BLIP, mention 'NA'.\\nAnswer: \"\n",
    "    gen_text = llamaPipe(prompt)\n",
    "    prompt_answers1.append(gen_text)\n",
    "# llamaPipe(\"What are the benefits of being vegetarian? Write a brief summary around the advantages and disadvantages of being vegetarian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'white'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ToDo \n",
    "### Write a function to evaluate the model performance as accuracy or the model\n",
    "\n",
    "### Experiment with prompt to provide a single word answer.\n",
    "\n",
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
