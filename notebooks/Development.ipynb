{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6625eb5-e8ca-4911-a33e-71f99d18ceed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices saved to 'indices.csv'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Image ID': [415885, 543551, 560463],\n",
       " 'Question ID': [415885005, 543551004, 560463001],\n",
       " 'Question': ['Is the man holding the tennis ball?',\n",
       "  'Where is this picture taken?',\n",
       "  'What color is the brick walkway?'],\n",
       " 'Answer': ['no', 'australia', 'green'],\n",
       " 'Plausible answers': [\"{'no'}\",\n",
       "  \"{'australia', 'sydney', 'seattle', 'by large lake', 'outside', 'bay', 'on shore', 'san francisco', 'by boardwalk'}\",\n",
       "  \"{'green', 'jade'}\"],\n",
       " 'Question Type': ['is the man', 'none of the above', 'what color is the'],\n",
       " 'Answer Type': ['yes/no', 'other', 'other'],\n",
       " 'Image file': ['../data/validation/images/COCO_val2014_000000415885.jpg',\n",
       "  '../data/validation/images/COCO_val2014_000000543551.jpg',\n",
       "  '../data/validation/images/COCO_val2014_000000560463.jpg'],\n",
       " 'Generated Caption': ['a man in white shorts and a blue shirt is playing tennis',\n",
       "  'a bus is parked on the side of the road',\n",
       "  'a man holding a tennis racket'],\n",
       " 'Generated Detections': [\"[{'xmin': 331.1903076171875, 'ymin': 344.2440185546875, 'xmax': 380.01495361328125, 'ymax': 428.19732666015625, 'confidence': 0.8386077284812927, 'class': 38, 'name': 'tennis racket'}, {'xmin': 0.0, 'ymin': 75.446533203125, 'xmax': 475.54010009765625, 'ymax': 600.5660400390625, 'confidence': 0.6917408108711243, 'class': 0, 'name': 'person'}]\",\n",
       "  \"[{'xmin': 290.159423828125, 'ymin': 189.49696350097656, 'xmax': 398.81109619140625, 'ymax': 323.3839416503906, 'confidence': 0.9088061451911926, 'class': 5, 'name': 'bus'}, {'xmin': 422.96844482421875, 'ymin': 237.23561096191406, 'xmax': 459.8575134277344, 'ymax': 263.4807434082031, 'confidence': 0.8443957567214966, 'class': 2, 'name': 'car'}, {'xmin': 341.02734375, 'ymin': 155.58236694335938, 'xmax': 403.39093017578125, 'ymax': 171.63037109375, 'confidence': 0.8116540908813477, 'class': 8, 'name': 'boat'}, {'xmin': 467.45416259765625, 'ymin': 239.48521423339844, 'xmax': 499.70025634765625, 'ymax': 266.1123352050781, 'confidence': 0.8009220361709595, 'class': 2, 'name': 'car'}, {'xmin': 308.28607177734375, 'ymin': 130.4901580810547, 'xmax': 338.49884033203125, 'ymax': 179.96746826171875, 'confidence': 0.6787567734718323, 'class': 8, 'name': 'boat'}, {'xmin': 164.05380249023438, 'ymin': 241.7511444091797, 'xmax': 176.77182006835938, 'ymax': 254.7535400390625, 'confidence': 0.5125336050987244, 'class': 0, 'name': 'person'}, {'xmin': 307.8382263183594, 'ymin': 168.8577117919922, 'xmax': 338.22686767578125, 'ymax': 179.09739685058594, 'confidence': 0.48807546496391296, 'class': 8, 'name': 'boat'}, {'xmin': 457.64666748046875, 'ymin': 230.30545043945312, 'xmax': 468.0733642578125, 'ymax': 255.7952117919922, 'confidence': 0.48116227984428406, 'class': 0, 'name': 'person'}, {'xmin': 205.37062072753906, 'ymin': 120.78675842285156, 'xmax': 230.98287963867188, 'ymax': 153.30166625976562, 'confidence': 0.47104841470718384, 'class': 8, 'name': 'boat'}, {'xmin': 171.5364990234375, 'ymin': 136.80690002441406, 'xmax': 190.9796142578125, 'ymax': 193.00082397460938, 'confidence': 0.46662047505378723, 'class': 8, 'name': 'boat'}, {'xmin': 205.67825317382812, 'ymin': 144.6035614013672, 'xmax': 230.30345153808594, 'ymax': 153.9208221435547, 'confidence': 0.3861176073551178, 'class': 8, 'name': 'boat'}, {'xmin': 422.15814208984375, 'ymin': 239.7126007080078, 'xmax': 499.8924255371094, 'ymax': 264.2164001464844, 'confidence': 0.38093382120132446, 'class': 2, 'name': 'car'}, {'xmin': 172.2530517578125, 'ymin': 177.18919372558594, 'xmax': 189.77743530273438, 'ymax': 193.06353759765625, 'confidence': 0.36907684803009033, 'class': 8, 'name': 'boat'}, {'xmin': 451.95074462890625, 'ymin': 234.8479461669922, 'xmax': 466.1075439453125, 'ymax': 261.4833679199219, 'confidence': 0.27001720666885376, 'class': 0, 'name': 'person'}]\",\n",
       "  \"[{'xmin': 82.37507629394531, 'ymin': 70.97930908203125, 'xmax': 268.33697509765625, 'ymax': 467.9751892089844, 'confidence': 0.8596909046173096, 'class': 0, 'name': 'person'}, {'xmin': 23.572811126708984, 'ymin': 126.4344711303711, 'xmax': 106.1563949584961, 'ymax': 278.73712158203125, 'confidence': 0.8023232817649841, 'class': 38, 'name': 'tennis racket'}]\"]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline\n",
    "from huggingface_hub import login\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "import torch\n",
    "\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "detections_file = \"../data/10k_yolo_detections.csv\"\n",
    "samples = 100\n",
    "\n",
    "detections_data = pd.read_csv(detections_file)\n",
    "data = detections_data\n",
    "\n",
    "indices = data.sample(n=samples, random_state = np.random.seed(42)).index.tolist()\n",
    "\n",
    "# Convert the list of indices to a DataFrame\n",
    "indices_df = pd.DataFrame(indices, columns=['Index'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "indices_df.to_csv('indices.csv', index=False)\n",
    "\n",
    "print(\"Indices saved to 'indices.csv'\")\n",
    "\n",
    "filtered_df = data.drop(indices)\n",
    "filtered_indices = filtered_df.index.tolist()\n",
    "\n",
    "dataset = Dataset.from_pandas(data)\n",
    "\n",
    "in_context_example_indices = np.random.choice(filtered_indices, 3)\n",
    "\n",
    "dataset[in_context_example_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "55a599d5-b5fb-493b-9c80-f360221757e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.66s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 60.88 MiB is free. Process 48379 has 56.70 GiB memory in use. Including non-PyTorch memory, this process has 22.37 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 5.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Llama-2-7b-chat-hf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm_gpu/lib/python3.10/site-packages/transformers/pipelines/__init__.py:1107\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1105\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m device\n\u001b[0;32m-> 1107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm_gpu/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:84\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_model_type(\n\u001b[1;32m     86\u001b[0m         TF_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n\u001b[1;32m     87\u001b[0m     )\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefix\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_params:\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;66;03m# This is very specific. The logic is quite complex and needs to be done\u001b[39;00m\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;66;03m# as a \"default\".\u001b[39;00m\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;66;03m# It also defines both some preprocess_kwargs and generate_kwargs\u001b[39;00m\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;66;03m# which is why we cannot put them in their respective methods.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/llm_gpu/lib/python3.10/site-packages/transformers/pipelines/base.py:874\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, image_processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[38;5;66;03m# We shouldn't call `model.to()` for models loaded with accelerate\u001b[39;00m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m hf_device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    873\u001b[0m ):\n\u001b[0;32m--> 874\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# Update config and generation_config with task specific parameters\u001b[39;00m\n\u001b[1;32m    877\u001b[0m task_specific_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtask_specific_params\n",
      "File \u001b[0;32m~/.conda/envs/llm_gpu/lib/python3.10/site-packages/transformers/modeling_utils.py:2576\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2572\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2573\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2574\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2575\u001b[0m         )\n\u001b[0;32m-> 2576\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm_gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm_gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/llm_gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 802 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/llm_gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/llm_gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.conda/envs/llm_gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 60.88 MiB is free. Process 48379 has 56.70 GiB memory in use. Including non-PyTorch memory, this process has 22.37 GiB memory in use. Of the allocated memory 21.96 GiB is allocated by PyTorch, and 5.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-2-7b-chat-hf\", device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c2c583-0b3c-41cb-b595-8dfd739848fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = [\"Based on the image caption (generated by BLIP model): {caption}, and Object detections (generated by YOLOv5): {detection}, Answer the question '{question}' \\n Answer: \",\n",
    "             \"Based on the image caption (generated by BLIP model): {caption} and Object detections (generated by YOLOv5): {detection}. \\n In plain simple English language without any emoticons or icons or font colors or punctuation marks. I strongly state do not repeat the question, prompt used, disclaimer, explanantion or anything apart from answer, that is just provide the answer in a single word in lowercase, the question: '{question}'. Remember if you are unable to answer the question based on the caption provided by BLIP, mention 'NA'.\\nAnswer: \"\n",
    "    ]\n",
    "\n",
    "def apply_template(template: str, texts: str):\n",
    "    templated_text = []\n",
    "\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    \n",
    "    for text in texts:\n",
    "        tmpltd_txt = template\n",
    "        tmpltd_txt = tmpltd_txt.replace(\"{dialogue}\",text)\n",
    "        templated_text.append(tmpltd_txt)\n",
    "\n",
    "    return templated_text\n",
    "\n",
    "\n",
    "\n",
    "def make_prompt(in_context_example_indices, test_example_index, template):\n",
    "    ### WRITE YOUR CODE HERE \n",
    "    icl_captions = dataset[in_context_example_indices]['Generated Caption']\n",
    "    icl_detections = dataset[in_context_example_indices]['Generated Detections']\n",
    "    icl_questions = dataset[in_context_example_indices]['Question']\n",
    "    icl_answers = dataset[in_context_example_indices]['Answer']\n",
    "\n",
    "    prompt = r\"\"\n",
    "    for caption, detection, question, answer in zip(icl_captions, icl_detections, icl_questions, icl_answers):\n",
    "        print(caption, detection, question, answer)\n",
    "        tmpltd_txt = template\n",
    "        tmpltd_txt = tmpltd_txt.replace(\"{caption}\",caption)\n",
    "        tmpltd_txt = tmpltd_txt.replace(\"{detection}\",detection)\n",
    "        tmpltd_txt = tmpltd_txt.replace(\"{question}\",question)\n",
    "        prompt += f\"\\n{tmpltd_txt}\\n\" + f\"{answer}\\n\\n\\n\"\n",
    "\n",
    "    test_caption = dataset[test_example_index]['Generated Caption']\n",
    "    test_detection = dataset[test_example_index]['Generated Detections']\n",
    "    test_question = dataset[test_example_index]['Question']\n",
    "    test_answer = dataset[test_example_index]['Answer']\n",
    "\n",
    "    tmpltd_txt = template\n",
    "    tmpltd_txt = tmpltd_txt.replace(\"{caption}\",test_caption)\n",
    "    tmpltd_txt = tmpltd_txt.replace(\"{detection}\",test_detection)\n",
    "    tmpltd_txt = tmpltd_txt.replace(\"{question}\",test_question)\n",
    "    \n",
    "    prompt += f\"\\n{tmpltd_txt}\\n\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc4b4d4-bb92-429d-80cf-4b1f1b56ec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for each_index in indices:\n",
    "    in_context_example_indices = np.random.choice(filtered_indices, 3)\n",
    "    template_prompt = make_prompt(in_context_example_indices, each_index, templates[0])\n",
    "    generated_txt = pipe(template_prompt, max_new_tokens=3)[0]['generated_text']\n",
    "    results.append(generated_txt)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0bacc9d6-4c85-4608-b519-61c4b99ecc4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the image caption (generated by BLIP model): {caption}, and Object detections (generated by YOLOv5): {detection}, Answer the question '{question}' \\n Answer: \""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "templates[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358ccd9e-7990-4d5a-b1ef-2dc2e5717f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
