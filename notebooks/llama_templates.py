# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kd-eh4YZRcaE9QeAI-tWzf9gz6L6dZ30
"""

# !pip install datasets
# !pip install transformers
# !pip install huggingface_hub
# !pip install pandas

import pandas as pd
from datasets import Dataset
from transformers import pipeline
from huggingface_hub import login
import time
import torch

detections_file = "../data/10k_yolo_detections.csv"
# detections_file = "10k_yolo_detections.csv"

df = pd.read_csv(detections_file)

templates = ["Based on the image caption(provided by BLIP model) as '{caption}' and detections(provided by Yolo) as {detections}, answer in a single word the question based on the image details as question: '{question}'\nAnswer: ",
             "Using the image caption '{caption}' and detected objects '{detections}', answer the following question with a single word: '{question}'.\nAnswer: ",
             "Caption: '{caption}'. Detected Objects: '{detections}'. What is the one-word answer to this question about the image: '{question}'?\nAnswer: ",
             "Given the description '{caption}' and identified elements '{detections}', provide a one-word response to this inquiry about the image: '{question}'.\nAnswer: ",
             "From the image caption '{caption}' and object detections '{detections}', find the answer to: '{question}'. Respond in just one word.\nAnswer: ",
             "Challenge: With the caption '{caption}' and objects detected as '{detections}', determine the single-word answer to the question: '{question}'.\nAnswer: ",
             "Answer in a single word for the question: {question} using image caption: {caption} and object detections: {detections}.\nAnswer: "]

login(token=token) ## This is bound to fail, add your token from chat and run


df = df[:1000]
for indx, template in enumerate(templates):
    if indx == 0:
        continue
    print(f"Running for {indx+1} template")
    
    # Record the start time
    start_time = time.time()
    
    df["Prompt"] = template
    df["Prompt"] = df.apply(lambda row: row['Prompt'].replace('{question}', row['Question']).replace('{detections}', row['Generated Detections']).replace('{caption}', row['Generated Caption']), axis=1)
    dataset = Dataset.from_pandas(df)
    pipe = pipeline("text-generation", model="meta-llama/Llama-2-7b-chat-hf", device='cuda', return_full_text=False)
    output = pipe(dataset["Prompt"], max_new_tokens=1)
    df.loc[:, "Model Output"] = output
    df.to_csv(f"Template_{indx}_1k_unquantized.csv", index=False)
    print(f"Successfully saved to Llama_Template_{indx}_1k_unquantized.csv")
    
    # Record the end time
    end_time = time.time()
    elapsed_time = end_time - start_time
    print(f"Elapsed time: {elapsed_time:.6f} seconds")






